# How and when to apply hyperparameter tuning in logistic regression ??

## First: kill a bad assumption

**Logistic Regression is NOT a hyperparameter-heavy model.**
If your model sucks, **tuning wonâ€™t save you**. Bad features, bad labels, bad class balance â€” tuning does nothing.

Hyperparameter tuning in logistic regression is **fine-tuning**, not resurrection.

---

## What hyperparameters even exist in Logistic Regression?

There are only **4 that actually matter**. Everything else is noise for beginners.

### 1ï¸âƒ£ Regularization type (`penalty`)

Controls **how** weights are penalized.

| Penalty      | Effect                                               |
| ------------ | ---------------------------------------------------- |
| `l2`         | Shrinks weights smoothly (default, safe)             |
| `l1`         | Forces some weights to exactly 0 (feature selection) |
| `elasticnet` | Mix of l1 + l2                                       |

If you donâ€™t know why you need `l1`, **you donâ€™t need it**.

---

### 2ï¸âƒ£ Regularization strength (`C`)

This is the **most important one**.

* `C` = inverse of regularization
* Small `C` â†’ strong regularization â†’ simpler model
* Large `C` â†’ weak regularization â†’ more flexible model

y = theta0 + theta1*x1 + theta2*x2 + ... In this equation if the values of theta0, theta1, theta2 are higher then we would get steep slope leading to overfitting. This can be prevented by `l2` penalty

In hyperparameter Tuning we dont change the theta0 value and we just try to change the theta1, theta2, theta3, etc values and due to this only the flat and steep slope can be achieved for best fit line. If we dont keep the theta0 value constant then there would not be any change in slope.

Think:

```
C â†“  â†’ model forced to be simple   (Reduces the theta values && The sigmoid function becomes less steep and tries to fit lesser and lesser data points)
C â†‘  â†’ model allowed to fit harder (Increases the theta values && The sigmoid function becomes more steep and tries to fit more and more data points)
```

---

#### What does penalty decide?

penalty decides the FORM of regularization, i.e. how coefficients are punished.

Examples:

* penalty="l2" â†’ penalize squared weights

* penalty="l1" â†’ penalize absolute weights

* penalty="elasticnet" â†’ mix of both


#### What does C decide?

C decides HOW STRONG the penalty is.

Mathematically (in sklearn):

Î» = 1 / C

example :- 
```
LogisticRegression(
    penalty="l2",
    C=0.1
)
```
Internally:

```
Penalty = Î» * (wâ‚Â² + wâ‚‚Â² + ...)
Î» = 1 / 0.1 = 10
```
---

### 3ï¸âƒ£ Solver

This decides **how optimization is done**, not what the model is.

| Solver      | Use case                    |
| ----------- | --------------------------- |
| `lbfgs`     | Default, fast, safe         |
| `liblinear` | Small datasets, supports l1 |
| `saga`      | Large datasets, elasticnet  |

Wrong solver = errors or slow training.

---

### 4ï¸âƒ£ Class weights (`class_weight`)

Critical for **imbalanced datasets**.

```
class_weight="balanced"  ## This means to give equal importance to both the classes
class_weight = { 0 : 10, 1 : 20 }  ## This means to give 10 times importance to class 0 and 20 times importance to class 1
```

This is often more impactful than tuning `C`.

---

## When should you tune Logistic Regression?

### âŒ Donâ€™t tune if:

* Accuracy is already poor
* Recall / precision is trash
* Data is imbalanced and untreated
* Features are unscaled

Fix the data first.

---

### âœ… Tune only when:

* Baseline model is reasonable
* You understand **what error matters**
* You have a validation strategy
* Model is underfitting or slightly overfitting

---

## How to decide WHAT to tune (logic, not guesswork)

### Step 1: Train a baseline model

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    penalty="l2",
    C=1.0,
    solver="lbfgs",
    max_iter=1000
)

model.fit(X_train, y_train)
```

Evaluate:

* Precision
* Recall
* F1
* Confusion matrix

---

### Step 2: Diagnose the problem

#### Case 1: Underfitting

* Low train score
* Low test score

ðŸ‘‰ **Increase C**

```python
C = 10, 50, 100
```

---

#### Case 2: Overfitting

* High train score
* Lower test score

ðŸ‘‰ **Decrease C**

```python
C = 0.1, 0.01, 0.001
```

---

#### Case 3: Too many useless features

ðŸ‘‰ Use `l1`

```python
penalty="l1"
solver="liblinear"
```

---

#### Case 4: Class imbalance

ðŸ‘‰ Use class weights **before anything else**

```python
class_weight="balanced"
```

---

## Proper way to tune (GridSearchCV)

### Example: clean, minimal grid

```python
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

param_grid = {
    "C": [0.01, 0.1, 1, 10, 100],
    "penalty": ["l2"],
    "solver": ["lbfgs"]
}

model = LogisticRegression(max_iter=1000)

grid = GridSearchCV(
    model,
    param_grid,
    scoring="f1",
    cv=5
)

grid.fit(X_train, y_train)

print(grid.best_params_)
```

âš ï¸ Notice:

* No insane grids
* No random nonsense
* Metric chosen **on purpose**

---

## Metric choice decides tuning outcome (critical)

If you tune with:

* `accuracy` â†’ model favors majority class
* `precision` â†’ avoids false positives
* `recall` â†’ avoids false negatives
* `f1` â†’ balance

**Wrong metric = wrong model.**

---

## What hyperparameter tuning CANNOT fix

Be clear about this:

âŒ Non-linear boundaries
âŒ Feature interactions
âŒ Label noise
âŒ Data leakage
âŒ Bad threshold choice

If you hit these â†’ logistic regression is the wrong tool.

---

## Production reality (listen carefully)

In real projects:

* `C` is often left at default
* `class_weight` is used heavily
* Threshold tuning (Using ROC Curve) beats hyperparameter tuning
* Feature engineering matters 10Ã— more

---

## Bottom line (no fluff)

* Tune logistic regression **only after baseline**
* `C` is the king
* Regularization > fancy grids
* Metrics matter more than parameters
* Data quality beats tuning every time

---

# How is Cross Validation used in Hyperparameter Tuning ??

## What GridSearchCV actually loops over

Suppose your grid is:

```python
param_grid = {
    "solver": ["liblinear", "lbfgs"],
    "penalty": ["l1", "l2"],
    "C": [0.1, 1, 10]
}
```

Total combinations:

```
2 solvers Ã— 2 penalties Ã— 3 C values = 12 configs
```

If `cv = 5`:

```
12 configs Ã— 5 folds = 60 model trainings
```

GridSearch **will train all 60**. No mercy.

---

## What happens for ONE combination

Take this specific combo:

```
solver = "liblinear"
penalty = "l1"
C = 1
```

GridSearch does:

| Fold | Train on    | Validate on | F1  |
| ---- | ----------- | ----------- | --- |
| 1    | 2â€“5         | 1           | f1â‚ |
| 2    | 1,3â€“5       | 2           | f1â‚‚ |
| 3    | 1,2,4,5     | 3           | f1â‚ƒ |
| 4    | 1â€“3,5       | 4           | f1â‚„ |
| 5    | 1â€“4         | 5           | f1â‚… |

Then:

```
mean_f1 = (f1â‚ + f1â‚‚ + f1â‚ƒ + f1â‚„ + f1â‚…) / 5
```

That **single scalar** represents this configuration.

---

## This repeats for EVERY combination

So internally, GridSearch builds a table like:

| solver    | penalty | C   | mean CV F1        |
| --------- | ------- | --- | ----------------- |
| liblinear | l1      | 0.1 | 0.69              |
| liblinear | l1      | 1   | 0.73              |
| liblinear | l1      | 10  | 0.70              |
| liblinear | l2      | 0.1 | 0.71              |
| lbfgs     | l2      | 1   | **0.76** â† winner |
| ...       | ...     | ... | ...               |

The **single best row wins**.

---

## Important constraint you must know (or youâ€™ll get errors)

Not all solverâ€“penalty combinations are valid.

Examples:

| Solver      | Supports           |
| ----------- | ------------------ |
| `lbfgs`     | l2 only            |
| `liblinear` | l1, l2             |
| `saga`      | l1, l2, elasticnet |

If you give an invalid combo, GridSearch will **crash**.

### Correct way to handle this

Use **multiple grids**, not one big wrong grid:

```python
param_grid = [
    {
        "solver": ["liblinear"],
        "penalty": ["l1", "l2"],
        "C": [0.1, 1, 10]
    },
    {
        "solver": ["lbfgs"],
        "penalty": ["l2"],
        "C": [0.1, 1, 10]
    }
]
```

This is not optional. This is correctness.

---



## Final mental model (lock this in)

* GridSearch = Cartesian product of params
* CV score = mean metric over folds
* Best config = highest mean score
* Final model = retrained on full training data with best config

---

## Interview-ready answer

> *GridSearchCV evaluates every valid combination of solver, penalty, and C using k-fold cross-validation.
> For each combination, it computes the mean validation score across folds and selects the configuration with the best average performance.*


### If the model is less accurate even after Hyperparameter Tuning then try to change the threshold using ROC Curve