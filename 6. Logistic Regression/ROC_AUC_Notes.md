# ROC

---

## 1Ô∏è‚É£ First: What Logistic Regression *actually* outputs (important)

Logistic Regression **does NOT output classes**.

It outputs **probabilities**:

```
P(y = 1 | x) = 0.0  ‚Üí  1.0
```

Example:

```
0.91  ‚Üí very confident positive
0.55  ‚Üí weak positive
0.49  ‚Üí weak negative
0.12  ‚Üí very confident negative
```

To convert this into a class, **you choose a threshold** (usually 0.5):

```
if prob ‚â• threshold ‚Üí class = 1
else ‚Üí class = 0
```

‚ö†Ô∏è **This threshold choice is where ROC comes in.**

---

## 2Ô∏è‚É£ The core problem ROC solves

Accuracy, precision, recall, F1‚Ä¶
üëâ **ALL of them depend on ONE fixed threshold**.

ROC answers a different question:

> ‚ÄúHow good is my model at separating positives from negatives **across ALL possible thresholds**?‚Äù

Not at 0.5
Not at 0.7
Not at 0.2

üëâ **Across every threshold from 0 to 1**

---

## 3Ô∏è‚É£ ROC Curve: what it really is

ROC = **Receiver Operating Characteristic**

It is a **curve**, not a single number.

### Axes (don‚Äôt memorize ‚Äî understand):

**X-axis:**
‚ùå False Positive Rate (FPR)

```
FPR = FP / (FP + TN)
```

Meaning:

> ‚ÄúOut of all actual negatives, how many did I incorrectly mark as positive?‚Äù

---

**Y-axis:**
‚úÖ True Positive Rate (TPR) = Recall

```
TPR = TP / (TP + FN)
```

Meaning:

> ‚ÄúOut of all actual positives, how many did I correctly catch?‚Äù

---

## 4Ô∏è‚É£ How the ROC curve is actually built (step-by-step)

Assume model outputs these probabilities:

| Probability | True Label |
| ----------- | ---------- |
| 0.95        | 1          |
| 0.85        | 1          |
| 0.70        | 0          |
| 0.55        | 1          |
| 0.40        | 0          |
| 0.20        | 0          |

Now vary threshold:

### Threshold = 0.9

Very strict
Only one positive predicted

‚Üí Low FP
‚Üí Low TP

### Threshold = 0.5

Balanced
More TP
Some FP

### Threshold = 0.1

Very loose
Almost everything positive

‚Üí High TP
‚Üí Very high FP

For **each threshold**, calculate:

```
(FPR, TPR)
```

Plot all those points
üëâ Join them
üëâ That‚Äôs the **ROC curve**

---

## 5Ô∏è‚É£ What a ‚Äúgood‚Äù ROC curve looks like

* Random model ‚Üí diagonal line
* Perfect model ‚Üí hugs top-left corner

Why top-left?

* **TPR = 1** (caught all positives)
* **FPR = 0** (no false alarms)

---

# AUC


## 1Ô∏è‚É£ What AUC is **NOT**

Let‚Äôs clear garbage first.

* ‚ùå It is **not accuracy**
* ‚ùå It is **not % correct predictions**
* ‚ùå It is **not about threshold = 0.5**
* ‚ùå It is **not a metric you ‚Äúcalculate from confusion matrix‚Äù**

If this is how you were thinking ‚Äî that‚Äôs the mistake.

---

## 2Ô∏è‚É£ The ONLY thing AUC cares about

AUC answers **one single question**:

> **Does the model give higher scores to positive samples than negative samples?**

That‚Äôs it.
No more. No less.

---

## 3Ô∏è‚É£ Think in real life terms (no ML)

Imagine this situation:

You are a **bouncer at a club**.

* Positive class = **drunk people** (should NOT enter)
* Negative class = **sober people** (can enter)

Your model gives a **‚Äúdrunk score‚Äù** from 0 to 1.

You don‚Äôt decide yet who enters.
You just **rank people** by how drunk they look.

---

## 4Ô∏è‚É£ One positive vs one negative (core idea)

Pick **one drunk person** and **one sober person**.

Now compare their scores.

### Case 1 ‚úÖ

```
Drunk person score  = 0.82
Sober person score  = 0.31
```

Model did a **good job**.

---

### Case 2 ‚ùå

```
Drunk person score  = 0.45
Sober person score  = 0.67
```

Model failed ‚Äî ranked wrong.

---

### Case 3 üòê

```
Drunk person score = 0.50
Sober person score = 0.50
```

Model is clueless ‚Äî tie.

---

## 5Ô∏è‚É£ THIS is how AUC is computed (mentally)

You **don‚Äôt need a curve** to understand AUC.

### Mental experiment:

1. Randomly pick **1 positive**

2. Randomly pick **1 negative**

3. Check:

   * Positive score > Negative score ‚Üí win
   * Positive score < Negative score ‚Üí loss
   * Equal ‚Üí half win

4. Repeat for **all possible pairs**

---

## 6Ô∏è‚É£ AUC = percentage of wins

That‚Äôs it.

If out of 100 such comparisons:

* 87 times positive scored higher
* 10 times negative scored higher
* 3 times tie

Then:

```
AUC = (87 + 0.5√ó3) / 100 = 0.885
```

üëâ **AUC ‚âà 0.89**

This is the **real meaning** of AUC.

---

## 7Ô∏è‚É£ Concrete numeric example (no hand-waving)

### Dataset

| Sample | Score | True Label |
| ------ | ----- | ---------- |
| A      | 0.9   | 1          |
| B      | 0.8   | 1          |
| C      | 0.6   | 0          |
| D      | 0.4   | 0          |

### All positive‚Äìnegative pairs

Positives: A, B
Negatives: C, D

Total pairs = 2 √ó 2 = 4

| Positive | Negative | Correct? |
| -------- | -------- | -------- |
| A(0.9)   | C(0.6)   | ‚úÖ        |
| A(0.9)   | D(0.4)   | ‚úÖ        |
| B(0.8)   | C(0.6)   | ‚úÖ        |
| B(0.8)   | D(0.4)   | ‚úÖ        |

```
AUC = 4 / 4 = 1.0 (perfect)
```

---

Now a bad model:

| Sample | Score | Label |
| ------ | ----- | ----- |
| A      | 0.6   | 1     |
| B      | 0.4   | 1     |
| C      | 0.9   | 0     |
| D      | 0.2   | 0     |

Pairs:

| Positive | Negative | Correct? |
| -------- | -------- | -------- |
| A(0.6)   | C(0.9)   | ‚ùå        |
| A(0.6)   | D(0.2)   | ‚úÖ        |
| B(0.4)   | C(0.9)   | ‚ùå        |
| B(0.4)   | D(0.2)   | ‚úÖ        |

```
AUC = 2 / 4 = 0.5
```

üëâ **Random guessing**

---

## 8Ô∏è‚É£ Why the curve exists at all (now it‚Äôll make sense)

ROC curve is just a **geometric trick** to compute this ranking score efficiently.

But conceptually:

> **AUC = how often positives outrank negatives**

That‚Äôs the soul.

---

## 9Ô∏è‚É£ Why AUC ignores threshold (important insight)

Because **ranking doesn‚Äôt need threshold**.

You‚Äôre only asking:

> ‚ÄúWho got a higher score?‚Äù

Not:

> ‚ÄúAbove 0.5 or not?‚Äù

That‚Äôs why AUC is stable.

---

## üîü When your brain should say ‚ÄúUse AUC‚Äù

Use AUC when:

* Model outputs **scores/probabilities**
* You care about **ordering**, not final decision
* Threshold is flexible or undecided

---

## Final one-liner (burn this in memory)

> **AUC is the probability that your model ranks a random positive higher than a random negative.**

---

The threshold where we will find High TPR and Less FPR will be selected for our logistic regression model.

ROC is NOT a decision tool. It‚Äôs a visualization tool.

Threshold selection depends on:

Cost of FP vs FN

Legal / medical / financial risk

User experience

---

How thresholds are ACTUALLY chosen using ROC

There are three legit ways. Anything else is hand-waving.

1Ô∏è‚É£ Distance to top-left (what you‚Äôre thinking)

We define distance:

    distance=sqrt((FPR‚àí0)^2+(TPR‚àí1)^2)
‚Äã

Pick threshold with minimum distance.

‚úî Works when:

FP and FN are equally bad

No business preference

‚ö† Weak in real life because costs are rarely equal.

2Ô∏è‚É£ Youden‚Äôs J statistic (cleaner and common)

J=TPR‚àíFPR

Pick threshold that maximizes J.

Why this works:

Maximizes separation between classes

Same intuition as top-left, but cleaner

This is the most commonly cited ROC-based threshold rule.

3Ô∏è‚É£ Business-driven threshold (most realistic)

Example:

Fraud detection:

FN is deadly

FP is acceptable

Then you don‚Äôt chase top-left.

You choose:

TPR ‚â• 0.95

Even if FPR becomes high

ROC just helps you see what threshold achieves that.